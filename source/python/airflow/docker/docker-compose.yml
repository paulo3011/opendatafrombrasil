version: '3.7'
services:
    # https://hub.docker.com/_/postgres
    postgresql:
        container_name: opendata-airflow-pgsql
        image: postgres:13
        environment:
            - POSTGRES_DB=airflow_db
            - POSTGRES_USER=airflow_user
            - POSTGRES_PASSWORD=airflow_pass
        #volumes:
            # - ./initdb/postgres:/docker-entrypoint-initdb.d
        logging:
            options:
                max-size: 10m
                max-file: "3"
        ports:
            - 5432:5432
        networks:
          - opendata

    airflow_webserver:
        container_name: opendata-airflow-v2.0.1
        image: moreira-opendata-airlfow-v2
        volumes:
            # You can also embed your dags in the image by simply adding them with COPY directive of Airflow. The DAGs in production image are in /opt/airflow/dags folder.
            - ../runtime/dags:/opt/airflow/dags
            - ../runtime/plugins:/opt/airflow/plugins
        #command: bash -c "{echo starting scheduler; airflow scheduler} & {airflow webserver;}"
        command: bash -c "airflow db init && airflow webserver"
        #command: webserver
        depends_on:
            - postgresql
        environment:
            # https://airflow.apache.org/docs/apache-airflow/stable/howto/set-config.html
            - AIRFLOW__CORE__LOAD_EXAMPLES=false
            # https://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html
            - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow_user:airflow_pass@postgresql:5432/airflow_db
            - AIRFLOW__CORE__EXECUTOR=LocalExecutor
            # How often (in seconds) to scan the DAGs directory for new files. Default to 5 minutes.
            - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=15
            - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
            - _AIRFLOW_DB_UPGRADE=true
            - _AIRFLOW_WWW_USER_CREATE=true
            # Create a default user for airflow ui with pass and login: airflow
            - _AIRFLOW_WWW_USER_USERNAME=${_AIRFLOW_WWW_USER_USERNAME:-airflow}
            - _AIRFLOW_WWW_USER_PASSWORD=${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
            # will be necessary to run docker-compose down if you use this setting after one docker-compose up run
            - AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=false            
            # to generate the key, install locally with pip: https://pypi.org/project/cryptography/
            # seealso: https://stackoverflow.com/questions/53897333/read-fernet-key-causes-valueerror-fernet-key-must-be-32-url-safe-base64-encoded
            - AIRFLOW__CORE__FERNET_KEY=G_F-iZ8J4zLE0nJ58DVplD0sma6m44ioTL8wWc3BU_k=
            # to give airflow credential do access aws (TAKE CARE: DON'T COMMIT YOUR CREDENTIALS):
            #- AWS_DEFAULT_REGION=us-east-1
            #- AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY
            #- AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEY
            #- AWS_SESSION_TOKEN=YOUR_TEMP_TOKEN_HERE
            # same personal database connection
            # The environment variable naming convention is AIRFLOW_CONN_{CONN_ID}, all uppercase.
            # seealso: https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html
            # to get the uri, you can use airflow cli, after manually create de connection: airflow connections get -o json mssql_conn
            # https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html#generating-connection-uri
            # ATTENTION: Connections set using Environment Variables would not appear in the Airflow UI but you will be able to use them in your DAG file.
            - AIRFLOW_CONN_MSSQL_CONN=mssql://your_user:your_pass@your_host:1433/databasename
            - AIRFLOW_CONN_S3_DEV=s3://your-bucket-name        
        ports:
            - "8081:8080"
        networks:
          - opendata

# https://docs.docker.com/compose/networking/
networks:
    opendata:
